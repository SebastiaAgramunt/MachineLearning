{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoencoder basically consists on compressing the data in one part of the neural network and decompress it in a second part. The idea is that one person has the trained encoder and another the trained autoencoder and the former sends compress information to the latter. The latter is able to recompose part of the image (while adding some noise because of the compression).\n",
    "\n",
    "This autoencoder will consist on 2 layers for the encoder and two more layers for the decoder. Both will have the same number of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "#create tmp folder if does not exist\n",
    "if not os.path.isdir('./tmp'):\n",
    "    os.mkdir('./tmp')\n",
    "\n",
    "# Download the dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"../MNIST/MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(X,Y,batch_size):\n",
    "    '''\n",
    "    Get batches of data, input is features,target\n",
    "    and batch_size, output is one minibatch\n",
    "    '''\n",
    "    iters = X.shape[0]//batch_size\n",
    "    for i in range(0,iters):\n",
    "        yield X[i*batch_size:(i+1)*batch_size],Y[i*batch_size:(i+1)*batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.01\n",
    "epochs = 150\n",
    "batch_size = 256\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "num_input = mnist.train.images.shape[1] # input = 28*28 = 784\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [None, num_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = 256 #number of cells for first layer\n",
    "n2 = 64 #number of cells for second layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the variables we are going to use\n",
    "W1_encoder = tf.Variable(tf.truncated_normal([num_input,n1]))\n",
    "W2_encoder = tf.Variable(tf.truncated_normal([n1,n2]))\n",
    "\n",
    "b1_encoder = tf.Variable(tf.truncated_normal([n1]))\n",
    "b2_encoder = tf.Variable(tf.truncated_normal([n2]))\n",
    "\n",
    "W1_decoder = tf.Variable(tf.truncated_normal([n2,n1]))\n",
    "W2_decoder = tf.Variable(tf.truncated_normal([n1,num_input]))\n",
    "\n",
    "b1_decoder = tf.Variable(tf.truncated_normal([n1]))\n",
    "b2_decoder = tf.Variable(tf.truncated_normal([num_input]))\n",
    "\n",
    "\n",
    "#here define the encoder and the decoder\n",
    "def encoder(x):\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x,W1_encoder),b1_encoder))\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1,W2_encoder),b2_encoder))\n",
    "    return layer_2\n",
    "\n",
    "def decoder(x):\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x,W1_decoder),b1_decoder))\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1,W2_decoder),b2_decoder))\n",
    "    return layer_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encode = encoder(X)\n",
    "Decode = decoder(Encode)\n",
    "\n",
    "y = Decode\n",
    "loss_op = tf.reduce_mean(tf.pow(tf.subtract(X,y),2.0)) #image has to be the same as the initial\n",
    "\n",
    "#We use AdamOptimizer, the GradientDescentOptimizer won't do it very well.\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss_op)\n",
    "\n",
    "#saver\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, batch-loss= 0.0864\n",
      "Epoch 1, batch-loss= 0.0745\n",
      "Epoch 10, batch-loss= 0.0519\n",
      "Epoch 20, batch-loss= 0.0460\n",
      "Epoch 30, batch-loss= 0.0434\n",
      "Epoch 40, batch-loss= 0.0439\n",
      "Epoch 50, batch-loss= 0.0405\n",
      "Epoch 60, batch-loss= 0.0400\n",
      "Epoch 70, batch-loss= 0.0396\n",
      "Epoch 80, batch-loss= 0.0396\n",
      "Epoch 90, batch-loss= 0.0358\n",
      "Epoch 100, batch-loss= 0.0363\n",
      "Epoch 110, batch-loss= 0.0354\n",
      "Epoch 120, batch-loss= 0.0366\n",
      "Epoch 130, batch-loss= 0.0351\n",
      "Epoch 140, batch-loss= 0.0349\n",
      "Testing Loss: 0.0386064\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        data = get_batches(X=mnist.train.images,Y=mnist.train.labels,batch_size=batch_size)\n",
    "        for x_,_ in data:\n",
    "            loss,_ = sess.run([loss_op,train_step],feed_dict = {X:x_})\n",
    "\n",
    "        if epoch % display_step == 0 or epoch == 1:\n",
    "            print(\"Epoch \" + str(epoch) + \", batch-loss= \" + \"{:.4f}\".format(loss))\n",
    "\n",
    "    # Calculate accuracy for 256 MNIST test images\n",
    "    print(\"Testing Loss:\", sess.run(loss_op, feed_dict={X: mnist.test.images}))\n",
    "    save_path = saver.save(sess, \"./tmp/autoencoder.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/QtBjK8zQzX+ygAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108f6b470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./tmp/autoencoder.ckpt\")\n",
    "    g = sess.run(Decode, feed_dict={X: mnist.test.images[6].reshape(-1,28*28)})\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.imshow(g.reshape(28,28),cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aind-dl]",
   "language": "python",
   "name": "conda-env-aind-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
